..
    Copyright (C) 2023-2025 Advanced Micro Devices, Inc. All rights reserved.

    Redistribution and use in source and binary forms, with or without modification,
    are permitted provided that the following conditions are met:
    1. Redistributions of source code must retain the above copyright notice,
       this list of conditions and the following disclaimer.
    2. Redistributions in binary form must reproduce the above copyright notice,
       this list of conditions and the following disclaimer in the documentation
       and/or other materials provided with the distribution.
    3. Neither the name of the copyright holder nor the names of its contributors
       may be used to endorse or promote products derived from this software without
       specific prior written permission.

    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
    ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
    WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
    IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
    INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
    BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
    OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
    WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
    ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
    POSSIBILITY OF SUCH DAMAGE.

.. _decision_forest_intro:

Decision Trees and Forests
*****************************

Decision trees introduction
=============================

Decision trees classify a data set by splitting the feature domain into rectangular regions and make predictions based on the label values in each region.
Each internal node of the tree conducts a test (e.g., evaluates whether feature :math:`j` is less than or equal to a defined threshold), leading to bifurcations
that direct data samples according to the outcomes of these tests. Ultimately, each leaf node of the tree holds a prediction, which takes the form of the majority
class label or the probabilities associated with each class derived from the training samples that reach that particular leaf.

The decision to split at a given node is taken based on its *impurity*, which measures the degree of uncertainty regarding the class labels among samples at that node.
It quantifies the extent to which the samples at a node are mixed in terms of their class labels. The available impurity metrics in AOCL-DA include Gini, misclassification error
and cross-entropy. In particular, a node exhibits an impurity of zero when all samples in the training set belong to a single class (a pure node).

The supported impurity measures are defined as:

.. math::

   \mathrm{Misclassification\ error: }   & \ 1 - \max(p_c)    \\
   \mathrm{Gini\ index: }                & \ 1 - \sum_{c} p_c^2  \\
   \operatorname{Cross-entropy or deviance: } &  - \sum_{c} p_c  \log_2(p_c)

where :math:`p_c` is the proportion of training samples with the label :math:`c`.

During the training process, decision tree algorithms seek to identify splits that yield the maximum reduction in impurity, achieved through the difference
between the impurity of the parent node and the weighted sum of the impurities of the child nodes. This iterative and greedy approach results in a structured
tree that effectively partitions the feature space into regions exhibiting low impurity, thereby facilitating efficient and interpretable predictions.


Decision forests introduction
=====================================

Decision forest fitting is performed by creating an ensemble of decision trees. This approach capitalizes on the diversity of the trees within the ensemble
to improve accuracy and reduce overfitting. The training data for each decision tree in the ensemble is generated by bootstrapping observations from the training data,
meaning they all train on a different subset of the training data generated through random sampling with replacement. Additionally, in every tree,
the best feature to split on at each internal node is determined by considereing a random subset of features rather than all available features.

The final prediction made by a random forest is obtained through majority voting across all the trees in the ensemble. This aggregation of predictions from numerous trees
leads to a more stable and accurate model compared to that of any single decision tree.


Histograms
============

AOCL-DA offers the option to work with histograms constructed from the training data rather than the original data set.
By discretizing continuous feature values into bins, histograms enable rapid calculations of impurity measures and split thresholds, thus reducing the computational overhead
associated with finding optimal splits during training. Furthermore, this histogram-based approach also helps to stabilize the model by smoothing out noise in the
data thus mitigating the risk of overfitting to individual data points.


Typical workflow for decision trees and decision forests
=========================================================

The following workflow can be used to fit a decision tree or a decision forest model and use it to make predictions,

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      **Decision trees**

      1. Initialize a :func:`aoclda.decision_tree.decision_tree()` object with options set in the class constructor.
      2. Fit the model using :func:`aoclda.decision_tree.decision_tree.fit`.
      3. Evaluate prediction accuracy on test data using :func:`aoclda.decision_tree.decision_tree.score`.
      4. Make predictions using the fitted model using :func:`aoclda.decision_tree.decision_tree.predict`.

      **Decision forests**

      1. Initialize a :func:`aoclda.decision_forest.decision_forest()` object with options set in the class constructor.
      2. Fit the model using the :func:`aoclda.decision_forest.decision_forest.fit`.
      3. Evaluate prediction accuracy on test data using :func:`aoclda.decision_forest.decision_forest.score`.
      4. Make predictions using the fitted model using :func:`aoclda.decision_forest.decision_forest.predict`.

   .. tab-item:: C
      :sync: C

      **Decision trees**

      1. Initialize a :cpp:type:`da_handle` with :cpp:type:`da_handle_type` ``da_handle_decision_tree``.
      2. Pass data to the handle using :ref:`da_tree_set_training_data_? <da_tree_set_training_data>`.
      3. Set optional parameters, such as maximum depth, using :ref:`da_options_set_? <da_options_set>`  (see
         :ref:`options section <opts_decisionforests>`).
      4. Fit the model using :ref:`da_tree_fit_? <da_tree_fit>`.
      5. Evaluate prediction accuracy on test data using :ref:`da_tree_score_? <da_tree_score>`.
      6. Make predictions using the fitted model using :ref:`da_tree_predict_? <da_tree_predict>`.

      **Decision forests**

      1. Initialize a :cpp:type:`da_handle` with :cpp:type:`da_handle_type` ``da_handle_decision_forest``.
      2. Pass data to the handle using :ref:`da_forest_set_training_data_? <da_forest_set_training_data>`.
      3. Set optional parameters, such as maximum depth, using :ref:`da_options_set_? <da_options_set>`  (see
         :ref:`options section <opts_decisionforests>`).
      4. Fit the model using :ref:`da_forest_fit_? <da_forest_fit>`.
      5. Evaluate prediction accuracy on test data using :ref:`da_forest_score_? <da_forest_score>`.
      6. Make predictions using the fitted model using :ref:`da_forest_predict_? <da_forest_predict>`.


Optional parameters
========================

Decision trees options
-------------------------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      The available Python options are detailed in the :func:`aoclda.decision_tree.decision_tree` and
      :func:`aoclda.decision_forest.decision_forest` class constructors.

   .. tab-item:: C
      :sync: C

      The following options can be set using :ref:`da_options_set_? <da_options_set>`:

      .. update options using table _opts_decisiontrees

      .. csv-table:: :strong:`Table of Options for Decision Trees.`
         :escape: ~
         :header: "Option name", "Type", "Default", "Description", "Constraints"

         "maximum bins", "integer", ":math:`i=256`", "Maximum number of bins in histograms.", ":math:`2 \le i \le 65535`"
         "histogram", "string", ":math:`s=` `no`", "Choose whether to use histograms constructed from the data matrix X.", ":math:`s=` `no`, or `yes`."
         "category split strategy", "string", ":math:`s=` `ordered`", "Strategy to split categorical features. For a given categorical feature, 'one-vs-all' tries to split each categorical value from all the the others while 'ordered' will try to split the smaller categories from the bigger ones.", ":math:`s=` `one-vs-all`, or `ordered`."
         "storage order", "string", ":math:`s=` `column-major`", "Whether data is supplied and returned in row- or column-major order.", ":math:`s=` `c`, `column-major`, `f`, `fortran`, or `row-major`."
         "check data", "string", ":math:`s=` `no`", "Check input data for NaNs prior to performing computation.", ":math:`s=` `no`, or `yes`."
         "feature threshold", "real", ":math:`r=1e-05`", "Minimum difference in feature value required for splitting.", ":math:`0 \le r`"
         "node minimum samples", "integer", ":math:`i=2`", "The minimum number of samples required to split an internal node.", ":math:`1 \le i`"
         "predict probabilities", "string", ":math:`s=` `yes`", "evaluate class probabilities (in addition to class predictions). Needs to be set to 'yes' if calls to predict_proba or predict_log_proba are made after fit.", ":math:`s=` `no`, or `yes`."
         "detect categorical data", "string", ":math:`s=` `no`", "Check if the data is categorical, encoded in [0, n_categories-1].", ":math:`s=` `no`, or `yes`."
         "scoring function", "string", ":math:`s=` `gini`", "Select scoring function to use.", ":math:`s=` `cross-entropy`, `entropy`, `gini`, `misclass`, `misclassification`, or `misclassification-error`."
         "maximum depth", "integer", ":math:`i=29`", "Set the maximum depth of trees.", ":math:`0 \le i \le 29`"
         "seed", "integer", ":math:`i=-1`", "Set the random seed for the random number generator. If the value is -1, a random seed is automatically generated. In this case the resulting classification will create non-reproducible results.", ":math:`-1 \le i`"
         "maximum features", "integer", ":math:`i=0`", "Set the number of features to consider when splitting a node. 0 means take all the features.", ":math:`0 \le i`"
         "minimum split score", "real", ":math:`r=0`", "Minimum score needed for a node to be considered for splitting.", ":math:`0 \le r \le 1`"
         "category tolerance", "real", ":math:`r=1e-05`", "How far data can be from an integer to be considered not categorical.", ":math:`0 \le r < 1`"
         "maximum categories", "integer", ":math:`i=50`", "Number of distinct values required for each feature for it to be considered categorical.", ":math:`2 \le i`"
         "minimum impurity decrease", "real", ":math:`r=0`", "Minimum score improvement needed to consider a split from the parent node.", ":math:`0 \le r`"




Decision forests options
----------------------------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      The available Python options are detailed in the :func:`aoclda.decision_tree.decision_tree` and
      :func:`aoclda.decision_forest.decision_forest` class constructors.

   .. tab-item:: C
      :sync: C

      The following options can be set using :ref:`da_options_set_? <da_options_set>`:

      .. update options using table _opts_decisionforests

      .. csv-table:: :strong:`Table of Options for Decision Forests.`
         :escape: ~
         :header: "Option name", "Type", "Default", "Description", "Constraints"

         "category split strategy", "string", ":math:`s=` `ordered`", "How to split categorical features: split one category from all other or consider them ordered.", ":math:`s=` `one-vs-all`, or `ordered`."
         "maximum bins", "integer", ":math:`i=256`", "Maximum number of bins in histograms.", ":math:`2 \le i \le 65535`"
         "histogram", "string", ":math:`s=` `no`", "Choose whether to use histograms constructed from the data matrix X.", ":math:`s=` `no`, or `yes`."
         "feature threshold", "real", ":math:`r=1e-05`", "Minimum difference in feature value required for splitting.", ":math:`0 \le r`"
         "storage order", "string", ":math:`s=` `column-major`", "Whether data is supplied and returned in row- or column-major order.", ":math:`s=` `c`, `column-major`, `f`, `fortran`, or `row-major`."
         "check data", "string", ":math:`s=` `no`", "Check input data for NaNs prior to performing computation.", ":math:`s=` `no`, or `yes`."
         "minimum split score", "real", ":math:`r=1e-05`", "Minimum score needed for a node to be considered for splitting.", ":math:`0 \le r \le 1`"
         "maximum features", "integer", ":math:`i=0`", "Set the number of features to consider when 'features selection' is set to 'custom'. 0 means take all the features.", ":math:`0 \le i`"
         "number of trees", "integer", ":math:`i=100`", "Set the number of trees to compute. ", ":math:`1 \le i`"
         "seed", "integer", ":math:`i=-1`", "Set random seed for the random number generator. If the value is -1, a random seed is automatically generated. In this case the resulting classification will create non-reproducible results.", ":math:`-1 \le i`"
         "node minimum samples", "integer", ":math:`i=2`", "Minimum number of samples to consider a node for splitting.", ":math:`1 \le i`"
         "maximum depth", "integer", ":math:`i=29`", "Set the maximum depth of trees.", ":math:`0 \le i \le 29`"
         "scoring function", "string", ":math:`s=` `gini`", "Select scoring function to use.", ":math:`s=` `cross-entropy`, `entropy`, `gini`, `misclass`, `misclassification`, or `misclassification-error`."
         "minimum impurity decrease", "real", ":math:`r=0`", "Minimum score improvement needed to consider a split from the parent node.", ":math:`0 \le r`"
         "block size", "integer", ":math:`i=256`", "Set the size of the blocks for parallel computations.", ":math:`1 \le i \le 2147483647`"
         "features selection", "string", ":math:`s=` `sqrt`", "Select how many features to use for each split. 'custom' reads the 'maximum features' option, proportion reads the 'proportion features' option. 'all', 'sqrt' and 'log2' select respectively all, the square root or the base-2 logarithm of the total number of features.", ":math:`s=` `all`, `custom`, `log2`, `proportion`, or `sqrt`."
         "bootstrap", "string", ":math:`s=` `yes`", "Select whether to bootstrap the samples in the trees.", ":math:`s=` `no`, or `yes`."
         "bootstrap samples factor", "real", ":math:`r=1`", "Proportion of samples to draw from the data set to build each tree if 'bootstrap' was set to 'yes'.", ":math:`0 < r \le 1`"
         "proportion features", "real", ":math:`r=0.1`", "Set the proportion of features to consider when 'features selection' is set to 'proportion'.", ":math:`0 < r \le 1`"



Choosing the decision forest options
------------------------------------

By default, the number of features to use in each split is set to be the square root of the total number of
features.  This can be changed through the ``features selection`` option.  If ``features
selection`` is set to ``custom``, then the value of ``maximum features`` will be used.
If ``features selection`` is set to ``proportion``, then the value of ``proportion features`` is used.
Otherwise, the value of the ``maximum features`` and ``proportion features`` options are ignored.

By default, bootstrap sampling is used, with the number of bootstrap samples set through the ``bootstrap samples
factor`` option.  However, if the value of the ``bootstrap`` option is set to ``no`` then no bootstrapping is
done, i.e., each tree uses the full dataset.

Additionally, histograms are turned on by default in decision forest classifiers as they typically speed up training significantly.
The maximum number of bins used for each feature is controlled by the option ``maximum bins``. Its default value of ``256`` is enough to not
lose accuracy for most data sets, but some may require higher granularity in the discretization of features. Note that a higher number of bins
can increase the computation time significantly, in which case it may be beneficial to turn histograms off entirely.

The optimal values of the optional parameters is typically problem dependent.  Cross-validation is typically used to tune options /
hyperparameters.


Examples
=========

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      The code below is supplied with your installation (see :ref:`Python examples <python_examples>`).

      .. collapse:: Decision tree Example

          .. literalinclude:: ../../python_interface/python_package/aoclda/examples/decision_tree_ex.py
              :language: Python
              :linenos:

   .. tab-item:: C
      :sync: C

      The code below can be found in ``decision_tree.cpp`` in the ``examples`` folder of your installation.

      .. collapse:: Decision Forest Example Code

         .. literalinclude:: ../../tests/examples/decision_tree.cpp
            :language: C++
            :linenos:

Further reading
=================

An introduction to decision trees and to decision forests can be found in Chapters 9 and 15 of :cite:t:`da_hastie`.



Decision tree and decision forest APIs
========================================

This chapter contains two sets of APIs, one for classification using a single :ref:`decision tree <da_decision_trees_apis>` and another
one for the ensemble method :ref:`decision forests<da_decision_forests_apis>` (also known as random forests).

.. _da_decision_trees_apis:

Decision trees
----------------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      .. autoclass:: aoclda.decision_tree.decision_tree(criterion='gini', seed=-1, max_depth=10, max_features=0, min_samples_split=2, build_order='breadth first', min_impurity_decrease=0.0, min_split_score=0.0, feat_thresh=1.0e-06, check_data=false)
         :members:

   .. tab-item:: C
      :sync: C

      .. _da_tree_set_training_data:

      .. doxygenfunction:: da_tree_set_training_data_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_set_training_data_d
         :project: da

      .. _da_tree_fit:

      .. doxygenfunction:: da_tree_fit_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_fit_d
         :project: da

      .. _da_tree_predict:

      .. doxygenfunction:: da_tree_predict_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_predict_d
         :project: da

      .. _da_tree_predict_proba:

      .. doxygenfunction:: da_tree_predict_proba_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_predict_proba_d
         :project: da

      .. _da_tree_predict_log_proba:

      .. doxygenfunction:: da_tree_predict_log_proba_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_predict_log_proba_d
         :project: da

      .. _da_tree_score:

      .. doxygenfunction:: da_tree_score_s
         :project: da
         :outline:
      .. doxygenfunction:: da_tree_score_d
         :project: da


.. _da_decision_forests_apis:

Decision forests
-----------------

.. tab-set::

   .. tab-item:: Python
      :sync: Python

      .. autoclass:: aoclda.decision_forest.decision_forest(criterion='gini', bootstrap=True, n_trees=100, features_selection='sqrt', max_features=0, seed=-1, max_depth=10, min_samples_split=2, build_order='breadth first', samples_factor=0.8, min_impurity_decrease=0.0, min_split_score=0.0, feat_thresh=1.0e-06, check_data=false)
         :members:

   .. tab-item:: C
      :sync: C

      .. _da_forest_set_training_data:

      .. doxygenfunction:: da_forest_set_training_data_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_set_training_data_d
         :project: da

      .. _da_forest_fit:

      .. doxygenfunction:: da_forest_fit_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_fit_d
         :project: da

      .. _da_forest_predict:

      .. doxygenfunction:: da_forest_predict_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_predict_d
         :project: da

      .. _da_forest_predict_proba:

      .. doxygenfunction:: da_forest_predict_proba_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_predict_proba_d
         :project: da

      .. _da_forest_predict_log_proba:

      .. doxygenfunction:: da_forest_predict_log_proba_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_predict_log_proba_d
         :project: da

      .. _da_forest_score:

      .. doxygenfunction:: da_forest_score_s
         :project: da
         :outline:
      .. doxygenfunction:: da_forest_score_d
         :project: da

